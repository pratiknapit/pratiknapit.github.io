---
title: "MLOps"
collection: talks
type: "Machine Learning Ops"
permalink: /talks/MLOps
venue: "In Production"
date: 2024-01-01
location: Sydney, Australia
---

Machine learning in production, from codeAcademy on youtube.

### ML In Production ###

Step 1: Data Collection
- Scope my project, gather the required data, and engineer features for the task.

Step 2: Training Model
- Modifying the data and model to improve the results.

Step 3: Deployment in Production
- Tracking data that might lead performance loss (drift etc.) and triggering re-training.

ML in production works in a cycle/loop. If the model changes or new data arrives, these changes need to be made and the production environment must be set up to allow this. 

MLOPs: Possible scenarios
- Model performance starts to decay, let's take example of fraud detection: assume you have training your model for fraud detection, let's say you have deployed it as well and you see model is giving incorrect predictions, so most probably the frauders changed their strategy to fraud. So you need to collect data and retrain. 
- We might need to reformulate our problem as it gets difficult to gather the data which we need. 
- Violation of assumption why we made during training.
- Business Objectives changes. 

*MLOps is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently*. *MLOps is defined as the extension of the DevOps methodology to include Machine Learning and Data Science assets as first-class citizens within the DevOps ecology*. 

### Other problems ###

Post deployment woes: accounting for latency. 53% of visits are abandoned if a mobile sit takes longer than 3 seconds to load. 

Post deployment woes: maintianing fairness. 

### Model centric vs Data Centric ###

Model-centric: hold the data fixed and iteratively improve the code/model.

Data-centric: hold the model fixed and iteratively improve the data. 

### What is the business problem we are trying to solve? ###

1. Identifying AI/ML Opportunities: AI/ML could be useful in the actual forecasting task, where it could analyse past sales data and market trends to predict future sales with higher accuracy than traditional methods.
2. Breaking down the Sales Forecasting Process: We decompose the sales forecasting process into its component tasks, such as data gathering, historical sales analysis, market trend analysis, and actual forecasting. 
3. Estimating the ROI of AI/ML Implementation: The ROI could be estimated by comparing the potential increase in sales and decrease in wasted resources due to improved forecasts, against the costs of developing and maintaining the AI/ML solution. 
4. Prioritising tasks for AI/ML Implementation
5. Styructuring the AI/ML Implementation

### Understanding the ML Canvas

**The ML Canvas**: A tool with ten blockis that helps us structure and plan our ML application development. 

1. Value Proposition: defining the problem, its importance and the end-user persona. 
- Geoffrey Moore's Value Positioning Statement Template: For (target customer), who(need), our (product/service) is (product category) that (benefit). 

2. Data Sources
- Identifying potential sources of data, including internal databases, APIs, open databases, etc.
- Considering hidden costs such as data storage and purchasing external data. 

3. Prediction taks
- Supervised, unsupervised, classification, regression, ranking.
- Think about input, output and model complexity.

4. Feature engineering
- Working with domain experts to extract features from raw data sources.

5. Offline evaluation
- Setting up metrics to evaluate system performance pre-deployment. 
- Understanding model prediction errors and their impacts. 

6. Decisions
- Using predictions to make decisions. How wil the end-user interact with our predictions?
- Possible hidden costs, etc.

7. Collecting data 
- Collecting new data for model re-training and preventing model decay.
- Cost considerations for data collection and the rol of humans for data labelling. 

8. Building models
- Deciding frequency of model re-training and associate hidden costs. 
- Planning for changes in tech stack and scaling. 

9. Live evaluation and monitoring
- Setting up metrics to track system performance post-deployment. 
- Understanding the correlation between model metrics and business metrics. 

10. When not to implement AI/ML: identify situations where AI/ML may not be the best solution.

### Workflow of Machine Learning-based software development ###

3 main artifacts in ML-based software: Data, ML Model, and Code.
3 main phases: Data Engineering, ML Model Engineering, and Code Engineering.

Data engineering pipeline:
- Data ingestion: collection of data from different sources
- Exploration and validation: understanding data content and structure
- Data wrangling: formatting and cleaning the data
- Data labelling: assigning categories to data points
- Data splitting: division of data into training, validation and test datasets. 

Model engineering: writing and executing ML algorithms
- Model training: applying ML algorithms on training data
- Model evaluation: validating the model pre-deployment
- Model testing: final acceptance test using test dataset
- Model packaging: exporting model into a consumable format for business application

Model deployment
- Model serving: addressing the ML model in a production environment
- Model performance monitoring: observing performance on live, unseen data
- Model performance logging: recording every inference request

## ZenML ##

ZenML follows a pipeline-based approach to organise ML workflows. This method promotes efficiency repeatability and collaboration in your projects. 


